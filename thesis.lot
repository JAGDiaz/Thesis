\contentsline {table}{\numberline {3.1}{\ignorespaces Hyperparamters of the DLDMD algorithm held constant for each training run on Duffing.\relax }}{18}{table.caption.7}%
\contentsline {table}{\numberline {3.2}{\ignorespaces ReLU activation function vs ELU activation function. Unlike ReLU, ELU is negative for $x < 0$. For some models, ELU can be advantageous if ReLU causes many of the neuron to ``die'' during training which means the weights associated with that neuron become 0, and those neurons do not contribute anything to output of the machine. Other activation functions can assuage this problem, but ELU can be made arbitrarily close to ReLU and so is quite popular.\relax }}{19}{table.caption.8}%
