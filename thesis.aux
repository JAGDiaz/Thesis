\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {schapter}{ABSTRACT}{v}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {schapter}{LIST OF TABLES}{vii}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {schapter}{LIST OF FIGURES}{viii}{Doc-Start}\protected@file@percent }
\citation{koopman}
\@writefile{toc}{\pagebreak [3]}
\@writefile{toc}{\contentsline {chaphd}{CHAPTER}{1}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\nopagebreak }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}INTRODUCTION}{1}{chapter.1}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{1}{INTRODUCTION}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is Dynamic Mode Decomposition?}{1}{section.1.1}\protected@file@percent }
\citation{schmid}
\citation{lago}
\citation{williams}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}What are Neural Networks?}{3}{section.1.2}\protected@file@percent }
\citation{rumelhart}
\citation{hornik}
\citation{oreilly}
\citation{theodoridis}
\citation{hollander}
\citation{lago}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}DLDMD and Its Limitations}{5}{section.1.3}\protected@file@percent }
\newlabel{eqn:loss function}{{1.20}{5}{INTRODUCTION}{equation.1.3.20}{}}
\newlabel{eqn:blah}{{1.23}{5}{INTRODUCTION}{equation.1.3.23}{}}
\citation{lago}
\citation{lago}
\citation{brunton}
\citation{lusch}
\citation{lago}
\citation{kramer}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Network Architecture of DLDMD}{6}{section.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Example of DLDMD newtork with $N_S = 2,\ N_O = 4,$ and $N_L = 3$ where every hidden layer has 16 neurons. This representation is meaningful in that it gives the name of each layer as they appear from left to right, but it is missing some key network features like the activation functions.\relax }}{7}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:network details}{{1.1}{7}{Example of DLDMD newtork with $N_S = 2,\ N_O = 4,$ and $N_L = 3$ where every hidden layer has 16 neurons. This representation is meaningful in that it gives the name of each layer as they appear from left to right, but it is missing some key network features like the activation functions.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Statement of Dilemma}{7}{section.1.5}\protected@file@percent }
\citation{shannon}
\citation{kullback}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}KULLBACK-LEIBLER DIVERGENCE}{9}{chapter.2}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:KLD}{{2}{9}{KULLBACK-LEIBLER DIVERGENCE}{chapter.2}{}}
\newlabel{eqn:normal}{{2.2}{9}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.0.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Basic Definitions}{9}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Entropy and density function change with respect to $\sigma ^2$. The base of the logarithm shown here is $e$, but the base itself is irrelevant due to the shared properties of all logarithms: that they increase monotonically and unbounded. As the variance increases we see higher entropy in the left plot and a flattening of the probability density in the right plot.\relax }}{10}{figure.caption.2}\protected@file@percent }
\newlabel{fig:entropy example}{{2.1}{10}{Entropy and density function change with respect to $\sigma ^2$. The base of the logarithm shown here is $e$, but the base itself is irrelevant due to the shared properties of all logarithms: that they increase monotonically and unbounded. As the variance increases we see higher entropy in the left plot and a flattening of the probability density in the right plot.\relax }{figure.caption.2}{}}
\newlabel{eqn:discrete KLD}{{2.4}{10}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.1.4}{}}
\newlabel{eqn:continuous KLD}{{2.6}{10}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.1.6}{}}
\citation{rosenblatt}
\citation{parzen}
\newlabel{eqn:normal divergence}{{2.7}{11}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.1.7}{}}
\newlabel{eqn:normal divergence2}{{2.8}{11}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Kullback-Leibler Divergence example with two normal distributions. As expected, the divergence decreases to 0 as $\sigma ^2 \to 1$ from either direction.\relax }}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:divergence example}{{2.2}{11}{Kullback-Leibler Divergence example with two normal distributions. As expected, the divergence decreases to 0 as $\sigma ^2 \to 1$ from either direction.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}How to build distributions (Non-parametric statistics)}{11}{section.2.2}\protected@file@percent }
\citation{epanechnikov}
\citation{botev}
\citation{terrell}
\newlabel{eqn:amise}{{2.10}{12}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Measuring entropy flow}{12}{section.2.3}\protected@file@percent }
\newlabel{eqn:weight convergence}{{2.11}{12}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a density function found using KDE, its shape coincides with that of the histogram generated from the same samples.\relax }}{13}{figure.caption.4}\protected@file@percent }
\newlabel{fig:kde example}{{2.3}{13}{Example of a density function found using KDE, its shape coincides with that of the histogram generated from the same samples.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Implementation Details}{13}{section.2.4}\protected@file@percent }
\citation{epanechnikov}
\newlabel{eqn:epanechnikov}{{2.16}{14}{KULLBACK-LEIBLER DIVERGENCE}{equation.2.4.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of linear fitting of divergence data for one of the models we examine in Chapter \ref  {chap:results}.\relax }}{16}{figure.caption.5}\protected@file@percent }
\newlabel{fig:divergence of model example}{{2.4}{16}{Example of linear fitting of divergence data for one of the models we examine in Chapter \ref {chap:results}.\relax }{figure.caption.5}{}}
\citation{lago}
\citation{strogatz:2000}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}RESULTS}{17}{chapter.3}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:results}{{3}{17}{RESULTS}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Duffing Oscillator}{17}{section.3.1}\protected@file@percent }
\newlabel{section:duffing results}{{3.1}{17}{RESULTS}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Duffing phase space.\relax }}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:duffing phase space}{{3.1}{18}{Duffing phase space.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Hyperparamters of the DLDMD algorithm held constant for each training run on Duffing.\relax }}{18}{table.caption.7}\protected@file@percent }
\newlabel{table:duffing params}{{3.1}{18}{Hyperparamters of the DLDMD algorithm held constant for each training run on Duffing.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces ReLU activation function vs ELU activation function. Unlike ReLU, ELU is negative for $x < 0$. For some models, ELU can be advantageous if ReLU causes many of the neuron to ``die'' during training which means the weights associated with that neuron become 0, and those neurons do not contribute anything to output of the machine. Other activation functions can assuage this problem, but ELU can be made arbitrarily close to ReLU and so is quite popular.\relax }}{19}{table.caption.8}\protected@file@percent }
\newlabel{table:elu vs relu}{{3.2}{19}{ReLU activation function vs ELU activation function. Unlike ReLU, ELU is negative for $x < 0$. For some models, ELU can be advantageous if ReLU causes many of the neuron to ``die'' during training which means the weights associated with that neuron become 0, and those neurons do not contribute anything to output of the machine. Other activation functions can assuage this problem, but ELU can be made arbitrarily close to ReLU and so is quite popular.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Duffing average slopes of linear fits with standard deviations. This plot illustrates what we want to find. Each latent dimension gives us a different behavior and we can readily see how the information must be changing during training.\relax }}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:duffing average slopes}{{3.2}{20}{Duffing average slopes of linear fits with standard deviations. This plot illustrates what we want to find. Each latent dimension gives us a different behavior and we can readily see how the information must be changing during training.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Duffing loss plots, smoothed to accentuate trends.\relax }}{21}{figure.caption.10}\protected@file@percent }
\newlabel{fig:duffing losses}{{3.3}{21}{Duffing loss plots, smoothed to accentuate trends.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Duffing Average bandwidth.\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:duffing average bandwidth}{{3.4}{21}{Duffing Average bandwidth.\relax }{figure.caption.11}{}}
\citation{strogatz:2000}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Van der Pol Oscillator}{22}{section.3.2}\protected@file@percent }
\newlabel{section:van der pol results}{{3.2}{22}{RESULTS}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Duffing slope of linear fits by layer.\relax }}{23}{figure.caption.12}\protected@file@percent }
\newlabel{fig:duffing slopes all layers}{{3.5}{23}{Duffing slope of linear fits by layer.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Duffing linear fits by layer.\relax }}{24}{figure.caption.13}\protected@file@percent }
\newlabel{fig:duffing linear fits all layers}{{3.6}{24}{Duffing linear fits by layer.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Duffing DLDMD results by latent dimension.\relax }}{25}{figure.caption.14}\protected@file@percent }
\newlabel{fig:duffing DLDMD results}{{3.7}{25}{Duffing DLDMD results by latent dimension.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Van der Pol phase space.\relax }}{26}{figure.caption.15}\protected@file@percent }
\newlabel{fig:van der pol phase space}{{3.8}{26}{Van der Pol phase space.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Van der Pol average slopes with standard deviations.\relax }}{27}{figure.caption.16}\protected@file@percent }
\newlabel{fig:van der pol average slopes}{{3.9}{27}{Van der Pol average slopes with standard deviations.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Van der Pol loss plots, smoothed to accentuate trend.\relax }}{27}{figure.caption.17}\protected@file@percent }
\newlabel{fig:van der pol losses}{{3.10}{27}{Van der Pol loss plots, smoothed to accentuate trend.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Van der Pol Average bandwidth.\relax }}{28}{figure.caption.18}\protected@file@percent }
\newlabel{fig:van der pol average bandwidth}{{3.11}{28}{Van der Pol Average bandwidth.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Van der Pol slope of linear fits by layer.\relax }}{29}{figure.caption.19}\protected@file@percent }
\newlabel{fig:van der pol slopes all layers}{{3.12}{29}{Van der Pol slope of linear fits by layer.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Van der Pol linear fits by layer.\relax }}{30}{figure.caption.20}\protected@file@percent }
\newlabel{fig:van der pol linear fits all layers}{{3.13}{30}{Van der Pol linear fits by layer.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Van der Pol DLDMD results by latent dimension.\relax }}{31}{figure.caption.21}\protected@file@percent }
\newlabel{fig:van der pol DLDMD results}{{3.14}{31}{Van der Pol DLDMD results by latent dimension.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}DISCUSSION}{32}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:discussion}{{4}{32}{DISCUSSION}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Good}{32}{section.4.1}\protected@file@percent }
\citation{lago}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Optimal Parameters}{33}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Non-unitary densities}{33}{section.4.3}\protected@file@percent }
\bibstyle{siammod}
\bibdata{main}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Statistical issues with edge layers}{34}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Future work}{34}{section.4.5}\protected@file@percent }
\bibcite{lago}{1}
\bibcite{botev}{2}
\bibcite{brunton}{3}
\bibcite{epanechnikov}{4}
\bibcite{oreilly}{5}
\bibcite{hollander}{6}
\bibcite{hornik}{7}
\bibcite{koopman}{8}
\bibcite{kramer}{9}
\bibcite{kullback}{10}
\bibcite{lusch}{11}
\bibcite{parzen}{12}
\bibcite{rosenblatt}{13}
\bibcite{rumelhart}{14}
\bibcite{schmid}{15}
\bibcite{terrell}{16}
\@writefile{toc}{\contentsline {schapter}{BIBLIOGRAPHY}{36}{section.4.5}\protected@file@percent }
\bibcite{shannon}{17}
\bibcite{strogatz:2000}{18}
\bibcite{theodoridis}{19}
\bibcite{williams}{20}
\gdef \@abspage@last{46}
