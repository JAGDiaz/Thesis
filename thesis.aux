\relax 
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {schapter}{ABSTRACT}{v}{}\protected@file@percent }
\@writefile{toc}{\contentsline {schapter}{LIST OF TABLES}{vii}{}\protected@file@percent }
\@writefile{toc}{\contentsline {schapter}{LIST OF FIGURES}{viii}{}\protected@file@percent }
\citation{koopman}
\@writefile{toc}{\pagebreak [3]}
\@writefile{toc}{\contentsline {chaphd}{CHAPTER}{1}{}\protected@file@percent }
\@writefile{toc}{\nopagebreak }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}INTRODUCTION}{1}{}\protected@file@percent }
\newlabel{chap:intro}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is E-DMD?}{1}{}\protected@file@percent }
\citation{lago}
\citation{williams}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}What are Neural Networks?}{3}{}\protected@file@percent }
\citation{hornik}
\citation{hollander}
\citation{lago}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}DLDMD and it's limitations}{5}{}\protected@file@percent }
\newlabel{eqn:loss function}{{1.1}{5}}
\newlabel{eqn:blah}{{1.4}{5}}
\citation{lago}
\citation{lago}
\citation{brunton}
\citation{lusch}
\citation{lago}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Statement of Dilemma}{6}{}\protected@file@percent }
\citation{shannon}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}KULLBACK-LEIBLER DIVERGENCE}{8}{}\protected@file@percent }
\newlabel{chap:KLD}{{2}{8}}
\newlabel{eqn:normal}{{2.2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Entropy and density function change with respect to $\sigma ^2$. The base of the logarithm shown here is $e$, but the base itself is irrelevant due to the shared properties of all logarithms: that they increase monotonically and unbounded. As the variance increases we see higher entropy in the left plot and a flattening of the probability density in the right plot.\relax }}{9}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:entropy example}{{2.1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Basic Definitions}{9}{}\protected@file@percent }
\newlabel{eqn:discrete KLD}{{2.4}{9}}
\newlabel{eqn:continuous KLD}{{2.6}{9}}
\newlabel{eqn:normal divergence}{{2.7}{10}}
\newlabel{eqn:normal divergence2}{{2.8}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces KL Divergence example with Normal distributions. As expected, the divergence decreases to 0 as $\sigma ^2 \to 1$ from either direction.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:divergence example}{{2.2}{10}}
\citation{epanechnikov}
\citation{botev}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}How to build distributions (Non-parametric statistics)}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Measuring entropy flow}{11}{}\protected@file@percent }
\newlabel{eqn:weight convergence}{{2.9}{11}}
\citation{epanechnikov}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Implementation Details}{12}{}\protected@file@percent }
\newlabel{eqn:epanechnikov}{{2.14}{12}}
\citation{epanechnikov}
\citation{epanechnikov}
\citation{botev}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of divergences for layer Dec 2 of Van der Pol model with latent dimension 8 with linear fitting and confidence bands.\relax }}{14}{}\protected@file@percent }
\newlabel{fig:divergence of model example}{{2.3}{14}}
\citation{lago}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}RESULTS}{16}{}\protected@file@percent }
\newlabel{chap:results}{{3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Duffing Oscillator}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Duffing phase space.\relax }}{17}{}\protected@file@percent }
\newlabel{fig:duffing phase space}{{3.1}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Van der Pol}{17}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Hyperparamters of the DLDMD algorithm held constant for each training run on Duffing.\relax }}{18}{}\protected@file@percent }
\newlabel{table:duffing params}{{3.1}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Lorenz (Maybe)}{18}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Duffing average slopes and loss plots.\relax }}{19}{}\protected@file@percent }
\newlabel{fig:duffing slopes and losses}{{3.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Duffing Average bandwidth.\relax }}{19}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Hyperparamters of the DLDMD algorithm held constant for each training run on Van der Pol.\relax }}{20}{}\protected@file@percent }
\newlabel{table:van der pol params}{{3.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Van der Pol phase space.\relax }}{21}{}\protected@file@percent }
\newlabel{fig:van der pol phase space}{{3.4}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Van der Pol average slopes and loss plots.\relax }}{21}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Van der Pol Average bandwidth.\relax }}{22}{}\protected@file@percent }
\bibstyle{siammod}
\bibdata{main}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}DISCUSSION}{23}{}\protected@file@percent }
\newlabel{chap:discussion}{{4}{23}}
\bibcite{lago}{1}
\bibcite{botev}{2}
\bibcite{brunton}{3}
\bibcite{epanechnikov}{4}
\bibcite{hollander}{5}
\bibcite{hornik}{6}
\bibcite{koopman}{7}
\bibcite{lusch}{8}
\bibcite{shannon}{9}
\bibcite{williams}{10}
\@writefile{toc}{\contentsline {schapter}{BIBLIOGRAPHY}{24}{}\protected@file@percent }
\gdef \@abspage@last{32}
